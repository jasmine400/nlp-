{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjiQPHe2j0NwcjZVtbVCEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasmine400/nlp-/blob/main/word_cloud_service3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymFE6P-EgYVY",
        "outputId": "85c59c64-6bcb-4ddb-f7fe-b2169c3a3d5c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pQU4woLgqss",
        "outputId": "2c1946d1-b942-4b9c-a140-913bbf61a5c7"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/meast\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/meast\n",
            "astan_real.csv\t\t kkrrbb44.png\t   resources\t     roshdanaNEW.png\n",
            "astan_real.gsheet\t koloka20.png\t   roshdana_0.ipynb  roshdana.png\n",
            "bkr.png\t\t\t koloka.png\t   roshdana2.csv     stagg.png\n",
            "collocation.ipynb\t krb2.ipynb\t   roshdana90.png    stopwords-fa.gdoc\n",
            "colooo.png\t\t krb.ipynb\t   roshdana91.png    stopwords-fa.txt\n",
            "dasti.ipynb\t\t laastclean.ipynb  roshdana92.png    testkdk.ipynb\n",
            "dictio.txt\t\t lastpo.png\t   roshdana98.png    tstk.png\n",
            "firsthalfspace.png\t lkr.png\t   roshdana.csv      Untitled1.ipynb\n",
            "firstremovestopword.png  lst2.png\t   roshdanaNEW1.png  Untitled3.ipynb\n",
            "kddk.png\t\t lst3.png\t   roshdanaNEW2.png\n",
            "kdk.ipynb\t\t lst.png\t   roshdanaNEW3.png\n",
            "kkrrbb2.png\t\t mob.ipynb\t   roshdanaNEW4.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkH3ooo4gtJI",
        "outputId": "c7308114-a1fd-4f27-c3fb-5c95cc3f5798"
      },
      "source": [
        "!pip install wordcloud-fa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wordcloud-fa\n",
            "  Downloading wordcloud_fa-0.1.8.tar.gz (76 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–                           | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 20 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 30 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 40 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 51 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 61 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 71 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud-fa) (1.19.5)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from wordcloud-fa) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from wordcloud-fa) (3.2.2)\n",
            "Collecting arabic_reshaper\n",
            "  Downloading arabic_reshaper-2.1.3-py3-none-any.whl (20 kB)\n",
            "Collecting python-bidi>=0.4.2\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Collecting wordcloud==1.7.0\n",
            "  Downloading wordcloud-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (364 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->wordcloud-fa) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->wordcloud-fa) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->wordcloud-fa) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->wordcloud-fa) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.1.2->wordcloud-fa) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper->wordcloud-fa) (57.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from arabic_reshaper->wordcloud-fa) (0.16.0)\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4 MB 31.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordcloud-fa, nltk, libwapiti\n",
            "  Building wheel for wordcloud-fa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordcloud-fa: filename=wordcloud_fa-0.1.8-py3-none-any.whl size=71015 sha256=f827be8c20019a371884941cffdd2d88002fd4f099c6c0536da681513cd7bdb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/a0/f3/81d930500493324929d26658e5a3ccff5ccc8644131ad1aadc\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=f61492277c424fbb0306922a77e06862e481762c29f049522bcb89505fd99b30\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154237 sha256=256dd938d0d6409ab0b4ab0fc0b629a7d60f745adb38dc33e4ebb2708a9f3535\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built wordcloud-fa nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, wordcloud, python-bidi, hazm, arabic-reshaper, wordcloud-fa\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: wordcloud\n",
            "    Found existing installation: wordcloud 1.5.0\n",
            "    Uninstalling wordcloud-1.5.0:\n",
            "      Successfully uninstalled wordcloud-1.5.0\n",
            "Successfully installed arabic-reshaper-2.1.3 hazm-0.7.0 libwapiti-0.2.1 nltk-3.3 python-bidi-0.4.2 wordcloud-1.7.0 wordcloud-fa-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rkzy4D0gxzQ",
        "outputId": "04ef40de-a86d-470e-ccda-e64bbc38a46c"
      },
      "source": [
        "pip install hazm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmwFF6eHg0yY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from wordcloud_fa import WordCloudFa\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PZiYaZDg9o4"
      },
      "source": [
        "tagger = POSTagger(model='resources/postagger.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHjj8YQihA6H"
      },
      "source": [
        "def edit(lst,tag , remain):\n",
        "  global i\n",
        "  c=[]\n",
        "  if lst==None :\n",
        "    return None\n",
        "  for i in (lst):\n",
        "   if (i[1]==tag and i[0]==remain):\n",
        "     b=[(i)]\n",
        "     c.extend(b)\n",
        "   if i[1]!=tag:\n",
        "     b=[(i)]\n",
        "     c.extend(b)\n",
        "\n",
        "  return(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdJJP4Gfi7rg"
      },
      "source": [
        "def wordListToFreqDict(rem):\n",
        "    wordfreq = [rem.count(p) for p in rem]\n",
        "    return dict(list(zip(rem,wordfreq)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvrTQeTVjGlY"
      },
      "source": [
        "def sortFreqDict(freqdict):\n",
        "    aux = [(freqdict[key], key) for key in freqdict]\n",
        "    aux.sort()\n",
        "    aux.reverse()\n",
        "    return aux"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAwcG9utjQLQ"
      },
      "source": [
        "def filterTheDict(dictObj, callback):\n",
        "    newDict = dict()\n",
        "\n",
        "    for (key, value) in dictObj.items():\n",
        "\n",
        "        if callback((key, value)):\n",
        "            newDict[key] = value\n",
        "    return newDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1ZXr5gXhHc5"
      },
      "source": [
        "def major(file_name):\n",
        "  df = pd.read_csv(file_name)\n",
        "  text=df['Comment'].values\n",
        "  a=['Ø¢']\n",
        "  bad_chars = [';', ':', '!', \"'\",\"[\",\",\",\"\\n\",\"ØŸ\",\".\",'\"',\"]\",'ğŸ™','âœ¨','ğŸŒ·','ğŸŒ¹','ğŸ˜˜','ğŸ˜”','ğŸ˜­','ğŸ™','ğŸ‘Œ','ğŸ˜','ğŸ‘','ğŸ‘','ğŸ’™','ğŸ˜','âœ…','ğŸŒº','ğŸ¥€','ğŸ˜¢','ğŸ˜','ğŸ˜','ğŸ','ğŸ“™','ğŸ˜ƒ','\\u200c','âœ']\n",
        "  for item in text:\n",
        "    b=str(item)\n",
        "    for i in bad_chars :\n",
        "      b = b.replace(i, '')\n",
        "    new=tagger.tag(word_tokenize(b))\n",
        "    rest=edit(new,'V','Ù†Ø±Ù…')\n",
        "    rest2=edit(rest,'PRO','Ù†Ø±Ù…')\n",
        "    rest3=edit(rest2,'PUNC','Ù†Ø±Ù…')\n",
        "    # print(rest3)\n",
        "    rest4=edit(rest3,'CONJ','Ù†Ø±Ù…')\n",
        "    rest5=edit(rest4,'ADV','Ù†Ø±Ù…')\n",
        "    rest6=edit(rest5,'P','Ù†Ø±Ù…')\n",
        "    rest7=edit(rest6,'PE','Ù†Ø±Ù…')\n",
        "    rest8=edit(rest7,'AJE','Ù†Ø±Ù…')\n",
        "    rest9=edit(rest8,'POSTP','Ù†Ø±Ù…')\n",
        "    if rest9==None :\n",
        "      n=1\n",
        "    else :\n",
        "      out = [item for t in rest9 for item in t]\n",
        "      a.extend(out)\n",
        "  # print(a)\n",
        "  ########\n",
        "  remove_list=['N','Ne','DET','AJ','NUM','RES','N','CL','AJe','Pe','DETe','ADVe','NUMe','PROe','INT','ADV','24','Û°Û¹Û³Û³Û¹Û±Û±Û¸Û´Û·Û²','Û°Û¹Û±ÛµÛ¸Û±Û¸Û·Û²Û±Û°','Û±Û³Û¶Û³','Ø¨Ø§Ø³Ù„Ø§Ù…']\n",
        "  remain = [i for i in a if i not in remove_list]\n",
        "  # print(remain)\n",
        "  #######\n",
        "  data= open('dictio.txt').read()\n",
        "\n",
        "  counter = 0\n",
        "  wordList = list()\n",
        "\n",
        "  for group in data.split('\\n'):\n",
        "          for word in group.split('\\t'):\n",
        "                  if (counter % 3 == 0):\n",
        "                          wordList.append(word)\n",
        "                  counter += 1\n",
        "  a=[\"Ø¢\"]\n",
        "\n",
        "  for i in range((len(wordList))) :\n",
        "    y=str(wordList[i])\n",
        "    y=y.split(\"\\u200c\")\n",
        "    a.extend(y)\n",
        "  # print(a)\n",
        "  # print(type(a))\n",
        "  res = [j for j in a if j not in wordList]\n",
        "  # print(res)\n",
        "  ###################\n",
        "  for j in range(len(remain)):\n",
        "    if remain[j] in res:\n",
        "      s=remain[j-1]+\"\\u200c\"+remain[j]\n",
        "      for i in range(len(wordList)):\n",
        "        if s==wordList[i]:\n",
        "          remain[j-1]=s\n",
        "          remain[j]=\" \"\n",
        "  ##################\n",
        "  new_word= open('stopwords-fa.txt').read()\n",
        "  new_word=new_word.split()\n",
        "  rem = [i for i in remain if i not in new_word]\n",
        "  # print(rem)\n",
        "  ############\n",
        "  a=wordListToFreqDict(rem)\n",
        "  # print(a)\n",
        "  #############\n",
        "  b=sortFreqDict(a)\n",
        "  # print(b)\n",
        "  #convert to dictionary\n",
        "  c=dict(b)\n",
        "  ###############\n",
        "  newDict = filterTheDict(c, lambda elem : elem[0] >50)\n",
        "  # print(newDict)\n",
        "  ####################reverse key and value in dictionary\n",
        "  reversed_dictionary = {value : key for (key, value) in c.items()}\n",
        "  # print(\"reversed key and value\")\n",
        "  # print(reversed_dictionary)\n",
        "  #############\n",
        "  #generate and save word cloud\n",
        "  wordcloud=WordCloudFa(width=1500, height=1300).generate_from_frequencies(reversed_dictionary)\n",
        "  img = wordcloud.to_image()\n",
        "  img.show\n",
        "  img.save('roshdana98.png')\n",
        "  return reversed_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctkXWl9fhgSq",
        "outputId": "9c5680b0-8f55-4b3e-9d77-ccf1cfca125a"
      },
      "source": [
        "\n",
        "name = input(\"Enter name file name : \")\n",
        "major(name)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter name file name : roshdana2.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 83,\n",
              " '1': 1,\n",
              " '100': 3,\n",
              " '16': 4,\n",
              " '19': 6,\n",
              " '3': 2,\n",
              " 'Ø¢Ø¯Ù…': 12,\n",
              " 'Ø¢Ø±Ø²ÙˆÛŒ': 14,\n",
              " 'Ø¢Ø²Ù…ÙˆÙ†': 7,\n",
              " 'Ø¢Ø´Ù†Ø§': 135,\n",
              " 'Ø¢Ø´Ù†Ø§ÛŒÛŒ': 39,\n",
              " 'Ø¢Ù…ÙˆØ²Ø´': 45,\n",
              " 'Ø¢Ù…ÙˆØ²Ø´ÛŒ': 32,\n",
              " 'Ø¢Ù…ÙˆØ²ÛŒ': 5,\n",
              " 'Ø¢Ù†Ù„Ø§ÛŒÙ†': 19,\n",
              " 'Ø¢ÛŒÙ†Ø¯Ù‡': 34,\n",
              " 'Ø§ØªÙØ§Ù‚': 15,\n",
              " 'Ø§ØªÙØ§Ù‚Ø§Øª': 8,\n",
              " 'Ø§ØªÙØ§Ù‚ÛŒ': 18,\n",
              " 'Ø§Ø­Ø³Ø§Ø³': 13,\n",
              " 'Ø§Ø¯Ù…': 10,\n",
              " 'Ø§Ø±Ø²Ø´Ù…Ù†Ø¯': 11,\n",
              " 'Ø§Ø³Ø§ØªÛŒØ¯': 46,\n",
              " 'Ø§Ø³ØªØ§Ø¯': 21,\n",
              " 'Ø§Ø³Ù…': 9,\n",
              " 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª': 37,\n",
              " 'Ø§ÙØ±Ø§Ø¯': 72,\n",
              " 'Ø§Ù†ØªØ®Ø§Ø¨': 54,\n",
              " 'Ø§Ù†Ø±Ú˜ÛŒ': 24,\n",
              " 'Ø§ÛŒØ¬Ø§Ø¯': 30,\n",
              " 'Ø¨Ø§Ø¨Øª': 26,\n",
              " 'Ø¨Ø§Ø²Ø§Ø±': 29,\n",
              " 'Ø¨Ø§Ø¹Ø«': 92,\n",
              " 'Ø¨Ø±Ú¯Ø²Ø§Ø±': 16,\n",
              " 'Ø¨Ù‚ÛŒÙ‡': 20,\n",
              " 'ØªØ¬Ø±Ø¨Ù‡': 114,\n",
              " 'ØªØ±Ù…': 23,\n",
              " 'ØªØ´Ú©Ø±': 61,\n",
              " 'ØªÙˆÙ†Ø³ØªÙ…': 22,\n",
              " 'ØªÛŒÙ…': 128,\n",
              " 'ØªÛŒÙ…ÛŒ': 33,\n",
              " 'Ø¬Ù†Ø§Ø¨': 49,\n",
              " 'Ø­Ø±ÙÙ‡': 35,\n",
              " 'Ø­Ø³': 31,\n",
              " 'Ø­Ø¶ÙˆØ±': 50,\n",
              " 'Ø­ÙˆØ²Ù‡': 77,\n",
              " 'Ø¯Ù†Ø¨Ø§Ù„': 28,\n",
              " 'Ø¯Ù†ÛŒØ§ÛŒ': 44,\n",
              " 'Ø¯ÙˆØ±Ù‡': 480,\n",
              " 'Ø±Ø´Ø¯': 117,\n",
              " 'Ø±Ø´Ø¯Ø§Ù†Ø§': 568,\n",
              " 'Ø±Ø´Ø¯Ø§Ù†Ø§ÛŒÛŒ': 17,\n",
              " 'Ø´Ø±ÙˆØ¹': 56,\n",
              " 'Ø¹Ø§Ù„ÛŒ': 74,\n",
              " 'Ø¹Ù„Ø§Ù‚Ù‡': 52,\n",
              " 'ÙØ±ØµØª': 67,\n",
              " 'ÙØ¶Ø§ÛŒ': 43,\n",
              " 'Ù…Ø¬Ù…ÙˆØ¹Ù‡': 63,\n",
              " 'Ù…Ø³ÛŒØ±': 151,\n",
              " 'Ù…Ù‡Ù†Ø¯Ø³': 53,\n",
              " 'Ù¾Ø±': 36,\n",
              " 'Ù¾ÙˆØ±Ø¹Ù„ÛŒ': 106,\n",
              " 'Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯': 25,\n",
              " 'Ú©Ø§Ø±': 207,\n",
              " 'Ú©Ø§Ø±Ø¢Ù…ÙˆØ²ÛŒ': 124,\n",
              " 'Ú©Ø³Ø¨': 125,\n",
              " 'Ú©Ù…Ú©': 68}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}